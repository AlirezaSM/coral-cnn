{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORAL Implementation Recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a brief overview of the relevant parts of CORAL that distinguish from a \"regular\" deep neural network for classification. The purpose of this overview is to provide a succinct resource that may help other researchers in porting the CORAL framework to other, non-PyTorch code bases (such as TensorFlow, Keras, MXnet, etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Implement a function that converts class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In contrast to regular cross entropy-based classification approaches, CORAL operates on binary tasks rather than integer class labels (or one-hot encoding representations thereof); hence, we have to convert class labels into the respective representation, which is illustrated here. \n",
    "- To provide an example, suppose you have a dataset consisting of 5 classes; consequently, the class labels are 0, 1, 2, 3, and 4.\n",
    "- The following `label_to_levels` function converts class labels into the binary task representation required by CORAL, we call them \"levels:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def label_to_levels(label, num_classes):\n",
    "    levels = [1]*label + [0]*(num_classes - 1 - label)\n",
    "    levels = torch.tensor(levels, dtype=torch.float32)\n",
    "    return levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To continue with the example, assume we have a dataset of 3 training examples with class labels 2, 1, 4\n",
    "- The converted labels would look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = 5\n",
    "\n",
    "levels = []\n",
    "class_labels = [2, 1, 4]\n",
    "\n",
    "for label in class_labels:\n",
    "    levels_from_label = label_to_levels(label, num_classes=NUM_CLASSES)\n",
    "    levels.append(levels_from_label)\n",
    "\n",
    "levels = torch.stack(levels)\n",
    "levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Implement the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As outlined in the paper, the loss function is defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\mathbf{W}, \\mathbf{b})=& \\\\\n",
    "&-\\sum_{i=1}^{N} \\sum_{k=1}^{K-1} \\lambda^{(k)}\\left[\\log \\left(s\\left(g\\left(\\mathbf{x}_{i}, \\mathbf{W}\\right)+b_{k}\\right)\\right) y_{i}^{(k)}\\right.\\\\\n",
    "&\\left.+\\log \\left(1-s\\left(g\\left(\\mathbf{x}_{i}, \\mathbf{W}\\right)+b_{k}\\right)\\right)\\left(1-y_{i}^{(k)}\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the paper, we used a uniform task importance weight $\\lambda$ (this means, all binary tasks were treated equally); this can be achieved by using a vectors of 1's as the task importance weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_weights = torch.ones(NUM_CLASSES-1, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The loss function itself, based on the equation above, can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def loss_fn1(logits, levels, imp):\n",
    "    val =  -torch.sum((torch.log(torch.sigmoid(logits))*levels + \n",
    "             torch.log(1 - torch.sigmoid(logits))*(1-levels))*imp,\n",
    "           dim=1)\n",
    "    return torch.mean(val)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To apply it to a concrete example, we use the previous \"levels\" and some made-up logit values (these logits values would be the neural network outputs).\n",
    "- Note that the rows represent the training examples, whereas the columns represent the logit value for each binary task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[2.1, 1.8, -2.1, -1.8],\n",
    "                       [1.9, -1., -1.5, -1.3],\n",
    "                       [1.9, 1.8, 1.7, 1.6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6920)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn1(logits=logits, \n",
    "         levels=levels,\n",
    "         imp=importance_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we found the loss function can be numerically more stable if we rewrite \n",
    "\n",
    "\n",
    "(1) \n",
    "\n",
    "```python\n",
    "torch.log(torch.sigmoid(logits))*levels\n",
    "```\n",
    "\n",
    "as \n",
    "\n",
    "```python \n",
    "F.logsigmoid(logits)*levels\n",
    "```\n",
    "\n",
    "and (2)\n",
    "\n",
    "```python \n",
    "torch.log(1 - torch.sigmoid(logits))*(1-levels)\n",
    "```\n",
    "\n",
    "as \n",
    "\n",
    "```python \n",
    "(F.logsigmoid(logits) - logits)*(1-levels)\n",
    "```\n",
    "\n",
    "Note that (2) if valid since\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "log\\bigg(\\frac{e^x}{1+e^x}\\bigg) - x &= log\\bigg(\\frac{e^x}{1+e^x}\\bigg) - log(e^x)\\\\\n",
    "&= log\\bigg(\\frac{1}{1+e^x}\\bigg)\\\\\n",
    "&= log\\bigg(1-\\frac{e^x}{1+e^x}\\bigg)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hence, in practice, we recommend using the following loss function (which produces the same results as the one outlined above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn2(logits, levels, imp):\n",
    "    val = (-torch.sum((F.logsigmoid(logits)*levels\n",
    "                      + (F.logsigmoid(logits) - logits)*(1-levels))*imp,\n",
    "           dim=1))\n",
    "    return torch.mean(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6920)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn2(logits=logits, \n",
    "         levels=levels,\n",
    "         imp=importance_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Modify the neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modification that has to be made to an existing deep neural network classifier is relatively simple as it only affects the last layer (i.e., output layer). In particular, the last fully connected layer has to be changed.\n",
    "\n",
    "In PyTorch, this means \n",
    "\n",
    "(1) changing the last fully connected layer\n",
    "\n",
    "```python\n",
    "...\n",
    "self.fc = nn.Linear(input_size, num_classes)\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```python\n",
    "...\n",
    "self.fc = nn.Linear(input_size, 1, bias=False)\n",
    "self.linear_1_bias = nn.Parameter(torch.zeros(self.num_classes-1).float())\n",
    "```\n",
    "\n",
    "(2) and changing the forward pass from\n",
    "\n",
    "```python\n",
    "...\n",
    "logits = self.fc(x)\n",
    "probas = F.softmax(logits, dim=1)\n",
    "return logits, probas\n",
    "```\n",
    "        \n",
    "to\n",
    "\n",
    "```python\n",
    "logits = self.fc(x) + self.linear_1_bias\n",
    "probas = torch.sigmoid(logits)\n",
    "return logits, probas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Evaluate the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing performance metrics such as the mean absolute error, or simply obtaining class labels, requires a small modification. Whereas in cross entropy-based classifiers, we obtain the class labels via\n",
    "\n",
    "```python\n",
    "logits, probas = model(features)\n",
    "_, predicted_labels = torch.max(probas, 1)\n",
    "```\n",
    "\n",
    "we can change these lines to\n",
    "\n",
    "```python\n",
    "logits, probas = model(features)\n",
    "predict_levels = probas > 0.5\n",
    "predicted_labels = torch.sum(predict_levels, dim=1)\n",
    "```\n",
    "\n",
    "in CORAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
